{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import utils.paths as plh\n",
    "import utils.read_utils as hlpread\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.models.feature_eng.TE_KFold import KFoldTargetEncoder\n",
    "from src.models.feature_eng.FreqEncoding import FrequencyEncoding\n",
    "from src.models.feature_eng.Combine_feature import CombineFeatures\n",
    "from src.models.feature_eng.KFoldFreqEncoding import KFoldFrequencyEncoding\n",
    "from src.models.feature_eng.CountVectorizerEncoding import CountVectorizerEncoding\n",
    "from src.models.feature_eng.TFIDFVectorizerEncoding import TFIDFVectorizerEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30391, 10)\n",
      "Index(['ACTION', 'RESOURCE', 'MGR_ID', 'ROLE_ROLLUP_1', 'ROLE_ROLLUP_2',\n",
      "       'ROLE_DEPTNAME', 'ROLE_TITLE', 'ROLE_FAMILY_DESC', 'ROLE_FAMILY',\n",
      "       'ROLE_CODE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ROOT = plh.get_project_root()\n",
    "\n",
    "train_data = os.path.join(PROJECT_ROOT, \n",
    "                          hlpread.read_yaml_key('data_source.data_folders'),\n",
    "                          hlpread.read_yaml_key('data_source.prepared.folder'),\n",
    "                          hlpread.read_yaml_key('data_source.prepared.clean_train'),\n",
    "                        )\n",
    "train = hlpread.read_from_parquet(train_data)\n",
    "\n",
    "print(train.shape)\n",
    "\n",
    "print(train.columns)\n",
    "#col_use = [x for x in train.columns if not x in ['ROLE_TITLE', 'MGR_ID']]\n",
    "#train = train[col_use]\n",
    "#targetcol = 'ACTION'\n",
    "\n",
    "#te_col = list(train.columns)\n",
    "#te_col.remove(targetcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ... (step 1 of 3) Processing combine_feature, total=   0.3s\n",
      "[Pipeline]  (step 2 of 3) Processing count_vectorizer_encoding, total= 2.4min\n",
      "[Pipeline] ........... (step 3 of 3) Processing KFoldTE, total=  46.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(30391, 233)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Order of the pipeline matter as this impact the output\n",
    "feature_engg = Pipeline( steps = [\n",
    "                                ('combine_feature', CombineFeatures()),\n",
    "\n",
    "                                #('frequency_encoding', FrequencyEncoding(min_group_size = 2)),\n",
    "\n",
    "                                #('tfidf_vectorizer_encoding', TFIDFVectorizerEncoding()),\n",
    "\n",
    "                                ('count_vectorizer_encoding', CountVectorizerEncoding()),\n",
    "\n",
    "                                ('KFoldTE', KFoldTargetEncoder()),\n",
    "\n",
    "                                ]\n",
    "                              ,verbose =  True\n",
    "                        )                        \n",
    "\n",
    "\n",
    "X = feature_engg.fit_transform(train) \n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the result of the dvc exp to cvs file \n",
    "\"\"\"\n",
    "import csv\n",
    "import subprocess\n",
    " \n",
    "## call date command ##\n",
    "p = subprocess.Popen(\"dvc exp show -A --csv\", stdout = subprocess.PIPE, shell = True)\n",
    "(output, err) = p.communicate()\n",
    "p_status = p.wait()\n",
    "\n",
    "with open('Example.csv', 'w', encoding='UTF8', newline='') as file:\n",
    "\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    header = []\n",
    "    data = []\n",
    "    for i, line in enumerate(output.splitlines()):\n",
    "\n",
    "        line = line.decode('ASCII') #Output return is  “byte string“. Note: ‘b‘ character before a string is used to specify the string as a “byte string“\n",
    "        \n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        \n",
    "        if len(header) == 0:\n",
    "            for word in line.split(','):\n",
    "                header.append(word)\n",
    "\n",
    "        else:\n",
    "            row = []    \n",
    "            for word in line.split(','):\n",
    "                row.append(word)\n",
    "\n",
    "            data.append(row)\n",
    "    \n",
    "    writer.writerow(header)\n",
    "    writer.writerow(data)\n",
    "\n",
    "\n",
    "#f = pd.DataFrame(data)\n",
    "#f.drop(range(146,198,1), axis = 1, inplace = True)\n",
    "#f.columns = header\n",
    "#f.shape\n",
    "#\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Models #\n",
    "\n",
    "Evaluate the models on the saved test dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = hlpread.read_object(\n",
    "                                    os.path.join(PROJECT_ROOT, \n",
    "                                                 hlpread.read_yaml_key('data_source.data_folders'),\n",
    "                                                 hlpread.read_yaml_key('model.trained_model')\n",
    "                                                 )\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predictions_by_class = trained_model.predict_proba(X).astype(float) #Return 2d numpy array which is the probaility for each class label\n",
    "Y = Y.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = Y_predictions_by_class.argmax(-1)  \n",
    "conf_matrix = confusion_matrix(Y, Y_pred)\n",
    "\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = hlpread.read_object(\n",
    "                                    os.path.join(PROJECT_ROOT, \n",
    "                                                 hlpread.read_yaml_key('data_source.data_folders'),\n",
    "                                                 hlpread.read_yaml_key('model.trained_model')\n",
    "                                                 )\n",
    ")\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predictions_by_class = trained_model.predict_proba(X).astype(float) #Return 2d numpy array which is the probaility for each class label\n",
    "Y = Y.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = Y_predictions_by_class.argmax(-1)  \n",
    "conf_matrix = confusion_matrix(Y, Y_pred)\n",
    "\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Decision Tree(Ensemble) ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = hlpread.read_object(\n",
    "                                    os.path.join(PROJECT_ROOT, \n",
    "                                                 hlpread.read_yaml_key('data_source.data_folders'),\n",
    "                                                 hlpread.read_yaml_key('model.trained_model')\n",
    "                                                 )\n",
    ")\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predictions_by_class = trained_model.predict_proba(X).astype(float) #Return 2d numpy array which is the probaility for each class label\n",
    "Y = Y.astype(float)\n",
    "\n",
    "Y_pred = Y_predictions_by_class.argmax(-1)  \n",
    "conf_matrix = confusion_matrix(Y, Y_pred)\n",
    "\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = hlpread.read_object(\n",
    "                                    os.path.join(PROJECT_ROOT, \n",
    "                                                 hlpread.read_yaml_key('data_source.data_folders'),\n",
    "                                                 hlpread.read_yaml_key('model.trained_model')\n",
    "                                                 )\n",
    ")\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predictions_by_class = trained_model.predict_proba(X).astype(float) #Return 2d numpy array which is the probaility for each class label\n",
    "Y = Y.astype(float)\n",
    "\n",
    "Y_pred = Y_predictions_by_class.argmax(-1)  \n",
    "conf_matrix = confusion_matrix(Y, Y_pred)\n",
    "\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation for Decision Tree model #\n",
    "\n",
    "Let's perform cross validation on the final parmater of the decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_param =  hlpread.read_yaml_key('trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load clean data\n",
    "import utils.read_utils as hlpread\n",
    "\n",
    "clean_train_data = os.path.join(PROJECT_ROOT,\n",
    "                                hlpread.read_yaml_key('data_source.data_folders'),\n",
    "                                hlpread.read_yaml_key('data_source.prepared.folder'),\n",
    "                                hlpread.read_yaml_key('data_source.prepared.clean_train'),\n",
    "                                )\n",
    "db_train = hlpread.read_from_parquet(clean_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "from src.models.feature_eng.CountVectorizerEncoding import CountVectorizerEncoding\n",
    "from src.models.feature_eng.Combine_feature import CombineFeatures\n",
    "from src.models.feature_eng.TE_KFold import KFoldTargetEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "feature_engg = Pipeline(steps = [\n",
    "                                    ('combine_feature', CombineFeatures()),\n",
    "                                    ('count_vectorizer_encoding', CountVectorizerEncoding()),\n",
    "                                    ('KFoldTE', KFoldTargetEncoder())\n",
    "                                ]) \n",
    "\n",
    "X = feature_engg.fit_transform(db_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define cross validation DT\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "cv_results = {}\n",
    "split_params = hlpread.read_yaml_key('train_test_split')\n",
    "split_s = StratifiedShuffleSplit(n_splits = 4, test_size = split_params['test_size'], random_state = split_params['random_seed'])\n",
    "\n",
    "fold = 0\n",
    "for train_index, test_index in split_s.split(X[X.columns[30:]], X['ACTION']):\n",
    "    \n",
    "    Y_train, Y_test = X[['ACTION']].iloc[train_index,:], X[['ACTION']].iloc[test_index,:]\n",
    "    X_train, X_test = X[X.columns[30:]].iloc[train_index,:], X[X.columns[30:]].iloc[test_index,:]\n",
    "\n",
    "    model = DecisionTreeClassifier(criterion = 'gini', random_state = 42)\n",
    "    model.set_params(**training_param['params'])\n",
    "\n",
    "    model.fit(X_train, Y_train ) \n",
    "\n",
    "    Y_test_pred = model.predict_proba(X_test).astype(float)\n",
    "    auc_score = roc_auc_score(Y_test.astype(float), Y_test_pred[:,1])\n",
    "\n",
    "    cv_results[fold] = auc_score\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(cv_results.values()) / len(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X[X.columns[30:]], X['ACTION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predictions_by_class = model.predict_proba(X[X.columns[30:]]).astype(float) #Return 2d numpy array which is the probaility for each class label\n",
    "Y = X['ACTION'].astype(float)\n",
    "\n",
    "Y_pred = Y_predictions_by_class.argmax(-1)  \n",
    "conf_matrix = confusion_matrix(Y, Y_pred)\n",
    "\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Y, Y_predictions_by_class[:,1])\n",
    "roc_auc_curve_df = pd.DataFrame()    \n",
    "roc_auc_curve_df['false_positive_rates'] = fpr\n",
    "roc_auc_curve_df['true_positive_rates'] = tpr\n",
    "roc_auc_curve_df['thresholds'] = thresholds\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (7.5, 7.5))\n",
    "ax.plot(roc_auc_curve_df['false_positive_rates'], roc_auc_curve_df['true_positive_rates'],  color = 'green', label = 'ROC Curve') #, marker = 'o'\n",
    "\n",
    "ax.tick_params(axis = 'both', labelcolor = 'green')\n",
    "ax.set_xlabel('False positive rate')\n",
    "ax.set_ylabel('True positive rate')\n",
    "label_str = str.format('ROC-AUC: {0}',  round(auc_score, 3))\n",
    "ax.text(0.5, 0, label_str, fontsize = 6)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bdc0d7d451ce7f6d5d2eccb7f4b3a7e5084394aa9c309b55e1eb76851fdd5e68"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
